# VEXORA the Log Recuration Assistant

## ğŸ“ Recent Updates (2025-12-18)

### ğŸ›  What Changed

- **Refactored LLM Client**: Converted `internal/llm/client.go` from a standalone script to a reusable package. Added a retry mechanism (3 attempts) to handle empty responses from Ollama.
- **Enhanced Database**: Added `GetEntriesByProject`, `GetEntryByID`, and `UpdateEntry` to `internal/database/db.go` to support filtering and updates.
- **New Web Dashboard**: Built a dark-mode web interface (`internal/api/web.go`, `templates/index.html`) using Tailwind CSS. Features include:
  - Project-specific filtering.
  - A "Raw Input" view with terminal-like styling.
  - A "Recurate" button to re-process entries with the AI.
  - A database table view for quick scanning.
- **Routing Updates**: Updated `cmd/main.go` to use Go 1.22+ routing for `/project/{project}` and `/recurate/{id}`.

### ğŸ§  Why

- The LLM client needed to be integrated into the main server, and local models can be flaky, so retries were essential.
- The web interface transforms the project from a backend API into a usable product for browsing logs.
- "Recuration" allows users to fix or improve AI outputs without manually editing the DB.

### ğŸ· Tags

## #golang #webdev #tailwindcss #sqlite #llm-integration

_Generated by Vexora Studio_

## ğŸ“ Recent Updates (2025-12-18) - Session 2

### ğŸ›  What Changed

- **Social Media Expansion**: Updated database schema and UI to support **LinkedIn** and **Instagram** drafts alongside Twitter.
- **Performance Metrics**: Added `processing_time` tracking to the database. The web UI now displays how long the AI took to generate each entry (e.g., "â±ï¸ 4.2s").
- **Markdown Support**: Integrated `marked.js` in the frontend to properly render the raw Markdown input notes.
- **Forking Capability**: Added a "Fork" feature (`/fork/{id}`) allowing users to create a new version of an entry based on the original raw notes, preserving history.
- **UI Polish**: Added specific icons and styling for the new social platforms and timing badges.

### ğŸ§  Why

- **Multi-Platform Content**: Developers often cross-post to LinkedIn and Instagram, not just Twitter.
- **Transparency**: Knowing how long the local LLM takes helps manage expectations and debug performance issues.
- **Content Iteration**: "Forking" allows for A/B testing different AI outputs or creating variations for different audiences without losing the original.

### ğŸ· Tags

## #feature-expansion #ux-improvement #social-media #performance-tracking

## ğŸ“ Recent Updates (2025-12-18) - Session 3

### ğŸ›  What Changed

- **LinkedIn Newsletter Generator**: Added a dedicated AI agent (`internal/api/newsletter.go`) that transforms rough notes into high-value, long-form newsletter editions.
- **Miscellaneous Mode**: Created a new `/misc` route and UI (`templates/misc.html`) for generating posts from scratch without a project context.
- **UX Enhancements**:
  - **Local Time Conversion**: Implemented JS logic to convert UTC database timestamps to the user's local browser time.
  - **Loading States**: Added a sleek, full-screen "Processing with AI..." overlay with a spinner for all AI actions (Recurate, Fork, Newsletter).
- **Database Schema**: Expanded `journal_entries` to store `newsletter_subject` and `newsletter_body`.

### ğŸ§  Why

- **Content Strategy**: Short posts are good, but long-form newsletters build deeper authority. The new agent automates this "Ghostwriter" workflow.
- **Flexibility**: Sometimes you just have a random thought that isn't part of a specific coding project. The "Misc" page handles this.
- **User Experience**: Waiting 10-30s for a local LLM without feedback feels broken. The loading overlay assures the user that work is happening.

### ğŸ· Tags

## #content-automation #newsletter-agent #ux-design #javascript #feature-complete

## ğŸ“ Recent Updates (2025-12-20)

### ğŸ›  What Changed

- **Multi-LLM Support**: Integrated **Google Gemini** as an alternative LLM provider alongside Ollama.
- **Provider Selection**: Added `LLM_PROVIDER` environment variable to switch between `ollama` (default) and `gemini`.
- **Gemini Integration**: Implemented a custom REST client for Gemini 1.5 Flash in `internal/llm/call_gemini.go`, supporting both text and JSON response modes.
- **Unified LLM Interface**: Refactored `internal/llm/gen_res.go` to use a unified `callLLM` function that routes requests based on the configured provider.

### ğŸ§  Why

- **Reliability & Speed**: While Ollama is great for local privacy, Gemini 1.5 Flash offers significantly faster generation and higher quality for complex tasks like newsletter generation.
- **Flexibility**: Users can now choose between a local-first (Ollama) or cloud-first (Gemini) approach depending on their hardware and needs.

### ğŸ· Tags

## #gemini #google-ai #multi-llm #ollama #refactoring
